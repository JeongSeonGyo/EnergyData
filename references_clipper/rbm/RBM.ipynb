{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Restricted Boltzmann Machine\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>\n",
    "#          Vlad Niculae\n",
    "#          Gabriel Synnaeve\n",
    "#          Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.utils.extmath import log_logistic\n",
    "from sklearn.utils.fixes import expit             # logistic function\n",
    "from sklearn.utils.validation import check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BernoulliRBM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Bernoulli Restricted Boltzmann Machine (RBM).\n",
    "\n",
    "    A Restricted Boltzmann Machine with binary visible units and\n",
    "    binary hiddens. Parameters are estimated using Stochastic Maximum\n",
    "    Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n",
    "    [2].\n",
    "\n",
    "    The time complexity of this implementation is ``O(d ** 2)`` assuming\n",
    "    d ~ n_features ~ n_components.\n",
    "\n",
    "    Read more in the :ref:`User Guide <rbm>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of binary hidden units.\n",
    "\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for weight updates. It is *highly* recommended\n",
    "        to tune this hyper-parameter. Reasonable values are in the\n",
    "        10**[0., -3.] range.\n",
    "\n",
    "    batch_size : int, optional\n",
    "        Number of examples per minibatch.\n",
    "\n",
    "    n_iter : int, optional\n",
    "        Number of iterations/sweeps over the training dataset to perform\n",
    "        during training.\n",
    "\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode.\n",
    "\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    intercept_hidden_ : array-like, shape (n_components,)\n",
    "        Biases of the hidden units.\n",
    "\n",
    "    intercept_visible_ : array-like, shape (n_features,)\n",
    "        Biases of the visible units.\n",
    "\n",
    "    components_ : array-like, shape (n_components, n_features)\n",
    "        Weight matrix, where n_features in the number of\n",
    "        visible units and n_components is the number of hidden units.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.neural_network import BernoulliRBM\n",
    "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    >>> model = BernoulliRBM(n_components=2)\n",
    "    >>> model.fit(X)\n",
    "    BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
    "           random_state=None, verbose=0)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    [1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for\n",
    "        deep belief nets. Neural Computation 18, pp 1527-1554.\n",
    "        http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\n",
    "\n",
    "    [2] Tieleman, T. Training Restricted Boltzmann Machines using\n",
    "        Approximations to the Likelihood Gradient. International Conference\n",
    "        on Machine Learning (ICML) 2008\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,\n",
    "                 n_iter=10, verbose=0, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            The data to be transformed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape (n_samples, n_components)\n",
    "            Latent representations of the data.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"components_\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float)\n",
    "        return self._mean_hiddens(X)\n",
    "\n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Computes the probabilities P(h=1|v).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        \"\"\"\n",
    "        p = safe_sparse_dot(v, self.components_.T)\n",
    "        p += self.intercept_hidden_\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def _sample_hiddens(self, v, rng):\n",
    "        \"\"\"Sample from the distribution P(h|v).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to sample from.\n",
    "\n",
    "        rng : RandomState\n",
    "            Random number generator to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer.\n",
    "        \"\"\"\n",
    "        p = self._mean_hiddens(v)\n",
    "        return (rng.random_sample(size=p.shape) < p)\n",
    "\n",
    "    def _sample_visibles(self, h, rng):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "\n",
    "        rng : RandomState\n",
    "            Random number generator to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_)\n",
    "        p += self.intercept_visible_\n",
    "        expit(p, out=p)\n",
    "        return (rng.random_sample(size=p.shape) < p)\n",
    "\n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the free energy F(v) = - log sum_h exp(-E(v,h)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        return (- safe_sparse_dot(v, self.intercept_visible_)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.components_.T)\n",
    "                               + self.intercept_hidden_).sum(axis=1))\n",
    "\n",
    "    def gibbs(self, v):\n",
    "        \"\"\"Perform one Gibbs sampling step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to start from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        v_new : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer after one Gibbs step.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"components_\")\n",
    "        if not hasattr(self, \"random_state_\"):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        h_ = self._sample_hiddens(v, self.random_state_)\n",
    "        v_ = self._sample_visibles(h_, self.random_state_)\n",
    "\n",
    "        return v_\n",
    "\n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : BernoulliRBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float)\n",
    "        if not hasattr(self, 'random_state_'):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        if not hasattr(self, 'components_'):\n",
    "            self.components_ = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='fortran')\n",
    "        if not hasattr(self, 'intercept_hidden_'):\n",
    "            self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'intercept_visible_'):\n",
    "            self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        if not hasattr(self, 'h_samples_'):\n",
    "            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X, self.random_state_)\n",
    "\n",
    "    def _fit(self, v_pos, rng):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Stochastic Maximum Likelihood (SML).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_pos : array-like, shape (n_samples, n_features)\n",
    "            The data to use for training.\n",
    "\n",
    "        rng : RandomState\n",
    "            Random number generator to use for sampling.\n",
    "        \"\"\"\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        v_neg = self._sample_visibles(self.h_samples_, rng)\n",
    "        h_neg = self._mean_hiddens(v_neg)\n",
    "\n",
    "        lr = float(self.learning_rate) / v_pos.shape[0]\n",
    "        update = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T\n",
    "        update -= np.dot(h_neg.T, v_neg)\n",
    "        self.components_ += lr * update\n",
    "        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.intercept_visible_ += lr * (np.asarray(\n",
    "                                         v_pos.sum(axis=0)).squeeze() -\n",
    "                                         v_neg.sum(axis=0))\n",
    "\n",
    "        h_neg[rng.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\n",
    "        self.h_samples_ = np.floor(h_neg, h_neg)\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes a quantity called the\n",
    "        free energy on X, then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"components_\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        # Randomly corrupt one feature in each sample in v.\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               rng.randint(0, v.shape[1], v.shape[0]))\n",
    "        if issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : BernoulliRBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float)\n",
    "        n_samples = X.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        self.components_ = np.asarray(\n",
    "            rng.normal(0, 0.01, (self.n_components, X.shape[1])),\n",
    "            order='fortran')\n",
    "        self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        verbose = self.verbose\n",
    "        begin = time.time()\n",
    "        for iteration in xrange(1, self.n_iter + 1):\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice], rng)\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
